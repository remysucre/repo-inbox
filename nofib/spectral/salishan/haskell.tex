\documentstyle[11pt]{article}
\textheight=8.0in
\textwidth=6.0in
\topmargin=0.5in
\oddsidemargin=0.5in
\evensidemargin=0.5in
\parskip=6pt plus2pt minus2pt

\newcommand{\lam}{\lambda}
\newcommand{\la}{\leftarrow}
\def\ab#1{\langle#1\rangle}

\begin{document}
\title{Haskell Solutions to the Language Session Problems\\
at the 1988 Salishan High-Speed Computing Conference}
\author{Paul Hudak\\
Steve Anderson\\[.2in]
Yale University\thanks{This research was supported primarily by DOE grant FG02-86ER25012.}\\
Department of Computer Science\\
New Haven, CT}
\maketitle

\section{Introduction}

Haskell is a new functional language, named after the logician Haskell
B. Curry, that was designed by a 14-member international committee
representative of the functional programming research community
\cite{haskell}.\footnote{The committee members are Arvind (MIT), Brian
Boutel (Victoria University of Wellington), Jon Fairbairn (Cambridge
University), Joseph Fasel (Los Alamos National Laboratory), Paul Hudak
(Yale University), John Hughes (Glasgow University), Thomas Johnsson
(Chalmers Institute of Technology), Dick Kieburtz (Oregon Graduate
Center), Rishuyar Nikhil (MIT), Simon Peyton-Jones (University College
London), Mike Reeve (Imperial College), Philip Wadler (Glasgow
University), David Wise (Indiana University), and Jonathan Young (Yale
and MIT).} The committee was formed because it was felt that research
and application of modern functional languages was being hampered by
the lack of a common language.  The committee's goals were that
Haskell should:
\begin{enumerate}
\item Be suitable for teaching, research, and applications, including 
      building large systems.
\item Be completely described via the publication of a formal syntax 
      and semantics.
\item Be freely available, such that anyone is permitted to implement 
      the language and distribute it to whomever they please.
\item Be based on ideas that enjoy a wide consensus.  
\item Be useable as a basis for further programming language research.  
\end{enumerate}

Haskell is a general purpose, purely functional programming language
exhibiting many of the recent innovations in programming language
research, including higher-order functions, non-strict functions and
data structures, static polymorphic typing, user-definable algebraic
data types, pattern-matching, list comprehensions, a module system,
and a rich set of primitive data types, including arbitrary and fixed
precision integers, and complex, rational, and floating-point numbers.
In addition it has several novel features that give it additional
expressivness, including an elegant form of overloading using a notion
of {\em type classes}, a flexible I/O system that unifies the two most
popular functional I/O models, and an array datatype that allows
purely functional, monolithic arrays to be constructed using ``array
comprehensions.''

The reader will note that we did not describe Haskell as a {\em
parallel} programming language; indeed it is not.  However, much
research in recent years has centered on the implementation of
functional languages on parallel machines, including the building of
special-purpose hardware such as dataflow and reduction machines.  We
will say little about these issues here, other than noting how the
solutions to the problems presented have considerable inherent
parallelism.

Given our space constraints, it is impossible for us to describe
Haskell in its entirety; our goal is only to give the reader some
familiarity with the language by giving solutions to the four language
session problems presented at the 1988 Salishan Conference on
High-Speed Computing.  The reader is referred to the Haskell Report
\cite{haskell} for a complete definition of the language.  Of course,
studying the solutions presented here will give the reader an idea of
what programming in any of a number of modern functional languages is
like; indeed, all of the solutions given have been run on our
implementation of Alfl, a functional language designed and implemented
at Yale.

\section{Brief Overview of Haskell}

In this section we will describe enough Haskell syntax to allow
understanding the programs given later.  As a result, there are
significant parts of Haskell that won't be described at all, most
notably user-defined data types, modules, and I/O.

Haskell is an ``equational'' language similar to
Miranda\footnote{Miranda is a trademark of Research Software Ltd.},
Hope, and several other modern functional languages.  A function is
defined by a set of equations which can {\em pattern-match} against
their arguments.  Lists are written {\tt [a,b,c]} with {\tt []} being
the empty list.  An element {\tt a} may be added to the front of the
list {\tt as} by writing {\tt a:as}.  Two lists may be appended
together by {\tt l1++l2}.  Here is an example of pattern-matching:
\begin{verbatim}
    member x []     = False
    '        (y:ys) = if x==y then True
                              else member x ys
\end{verbatim}
The ``tick mark'' on the second line is a convenient abbreviation for
the initial subsequence on the preceding line (so that the arity of
the two equations is the same).

A function {\tt f x = x+1} may also be defined ``anonymously'' with
the expression \verb! \x -> x+1!, and thus \verb! (\x -> x+1) 2!
returns {\tt 3}.

{\em List comprehensions} are a concise way to define lists, and are
best explained by example:
\begin{verbatim}
    [ (x,y) | x<-xs, y<-ys ]
\end{verbatim}
which constructs the list of all pairs whose first element is from
{\tt xs}, and second is from {\tt ys}.  ``Infinite lists'' may also be
defined, and thanks to lazy evaluation, only that portion of the list
that is needed by some other part of the program is actually computed.
Thus the infinite list of ones can be defined by:
\begin{verbatim}
    ones = 1 : ones
\end{verbatim}
The notation {\tt [a..b]} denotes the list of integers from {\tt a} to
{\tt b}, inclusive, and {\tt [a..]} is the infinite ascending list of
integers beginning with {\tt a}.

There are many standard utility functions defined on lists.  Aside
from {\tt member} defined earlier, the ones we need in this paper are
the following:
\begin{verbatim}
    takewhile pred []   = []       -- takes elements of list while pred is true
    '            (a:as) = if (pred a) then (a : takewhile pred as)
                                        else []

    foldl f a []   = a             -- folds list from left
    '       (x:xs) = foldl f (f a x) xs

    foldr f a []   = a             -- folds list from right
    '       (x:xs) = f x (foldr f a xs)

    zip   []     bs   = []         -- forms list of pairs from pair of lists
    '     as     []   = []
    '   (a:as) (b:bs) = (a,b) : zip as bs

    nodups []   = []               -- removes duplicates from list
    '    (x:xs) = x : nodups [ y | y <- xs, y /= x ]
\end{verbatim}
In Haskell, function application always has higher precedence than any
infix operator, and thus ``{\tt a : takewhile pred as}'' is parsed as
``{\tt a : (takewhile pred as)}.''  Note in {\tt zip} the use of {\em
tuples}, which in Haskell are constructed in arbitrary but finite
length by writing ``{\tt (a,b, ..., c)}'' (the parentheses are
mandatory); tuples may be pattern-matched like lists.  Finally, note
that for {\tt foldl} and {\tt foldr} the following relationships hold:
\begin{verbatim}
    foldl f a [x1, x2, ..., xn]  ==>  (f ... (f (f a x1) x2) ... xn)
    foldr f a [x1, x2, ..., xn]  ==>  (f x1 (f x2 ... (f xn a) ... ))
\end{verbatim}

Haskell also has {\em arrays} and a special syntax for manipulating
them.  A two-dimensional array {\tt a} is indexed at position {\tt
(i,j)} via the expression {\tt a!(i,j)}.  New arrays are constructed
using the primitive function {\tt array}, which takes a set of bounds
and a list comprehension as arguments; the list comprehension
specifies the set of index/value pairs for the new array.  For
example:
\begin{verbatim}
    array ((1,1),(n,n))
      [ ((i,j) , k*a!(i,j)) | i<-[1..n], j<-[1..n] ]
\end{verbatim}
returns a nXn matrix representing the matrix {\tt a} multiplied by the
scalar {\tt k}.

This description of Haskell is quite brief, but should be enough
to make the programs given later self-explanatory.  Nevetheless,
experience with at least one other functional language would be
beneficial.

\section{Hamming's Problem (Extended)}

\begin{quotation}
``Given as input a finite increasing sequence of primes
$\ab{a,b,c,...}$ and an integer $n$, output in order of increasing
magnitude and without duplication all integers less than or equal to
$n$ of the form:
\[ a^i b^j c^k...,\ \ \ i,j,k,...\geq 0 \]
Notice that if $m$ is in the output sequence then so are:
\[ am,\ bm,\ cm,\ ...\ \leq n \]
Our intention in posing the problem is to see how each language
expresses such mutually recursive stream computations.''
\end{quotation}

A natural way to solve this problem in Haskell is to generate an
infinite increasing sequence of hamming numbers, and then filter out
those less than $n$.  But how do we create that infinite sequence?  To
start, let's define a function {\tt scale} that multiplies every
element in a stream by a certain number:
\begin{verbatim}
    scale p xs = [ p*x | x<-xs ]
\end{verbatim}

\begin{figure}
\vspace{2.5in}
\caption{Naive Hamming Solution}
\label{h1}
\vspace{3.5in}
\caption{Hamming Solution Without Duplicates}
\label{h2}
\end{figure}

Now note that a constructive way to express the problem is as an
inductive definition:
\begin{itemize}
\item 1 is in the output sequence.
\item For each prime $p$, if $k$ is in the output sequence, then so is
      $k*p$.
\end{itemize}
We can construct a dataflow diagram for this as shown in Figure
\ref{h1}, where the repeating pattern has been highlighted in a
box.  Capturing the box's functionality in a function {\tt f}, and using
{\tt foldl} to ``unfold'' {\tt f} over the list of primes, we arrive at
this straightforward program to realize the dataflow diagram:
\begin{verbatim}
    hamming primes = 
        h where h = 1 : foldl f [] primes
                f xs p = merge xs (scale p h)
\end{verbatim}
where {\tt merge} merges a list of streams in increasing numeric
order.  Unfortunately, {\tt merge} must also {\em remove duplicates},
since this simple definition will construct every permutation of the
factors for a particular number.  For example, it will generate three
12's: 2*2*3, 2*3*2, and 3*2*2.  This is of course inefficient, and
we'd prefer a solution that avoided the extra multiplications.  

The problem stems from the fact that the sub-streams are generated
recursively from the {\em entire} list {\tt h}.  What we really want
is something that ``chases its tail'' so as to avoid generating all of
the combinations.  The dataflow diagram in Figure \ref{h2} in fact
does just that -- note how the result of each merge is fed back only
to itself, thus avoiding the duplicates.  As before we can express
this result by abstracting the repeating functionality and using {\tt
foldl}:
\begin{verbatim}
    hamming primes = 1 : foldl f [] primes
                     where f xs p = h where
                                    h = merge (scale p (1:h)) xs
\end{verbatim}
in which case {\tt merge} is defined simply by:
\begin{verbatim}
    merge (a:as) (b:bs) = if a<b then a : merge as (b:bs)
                                 else b : merge (a:as) bs
    '      [] bs = bs
    '      as [] = as
\end{verbatim}
and the result is just:
\begin{verbatim}
    takewhile (\x -> x<n) (hamming primes)
\end{verbatim}
using the utility {\tt takewhile} defined in the introduction.

Here is a sample output transcript, run on our Alfl implementation:
\begin{verbatim}
  takewhile (\x -> x<46) (hamming [2,3,5]);

  Result: [1,2,3,4,5,6,8,9,10,12,15,16,18,20,24,25,27,30,32,36,40,45]
\end{verbatim}

\section{The Paraffin Problem}

\begin{quotation}
``The chemical formula for paraffin molecules is $C_iH_{2i+2}$.  Given
an integer $n$, output without repetition and in order of increasing
size, structural representations of all paraffin molecules for $i \leq
n$.  Include all isomers, but no duplicates.  You may choose any
representation for the molecules you wish, so long as it clearly
distinguishes among isomers.''  The problem is discussed in:
\begin{quotation}
Turner, D. A., The semantic elegance of  applicative  languages.
Proc. Conf. on Functional Programming Languages and Computer Architecture, 
Portsmouth, NH, 1981 Oct., pp. 85-92.
\end{quotation}
\end{quotation}

This problem was solved in the above reference using the functional
language Miranda, which happens to be similar to Haskell, and thus our
job is already done for us!  Actually there are more efficient
algorithms for solving this problem, but no more insight into
understanding Haskell will be gained by giving them.  Thus we will
simply rewrite Turner's KRC solution in Haskell (we also made a few
simplifications), and refer to the paper referenced above for a
detailed description of it:

\begin{verbatim}
    main = foldr (++) [] (map paraffin [1..])

    paraffin n = quotient equiv [ [x,"H","H","H"] | x <- para (n-1) ]

    para = ["H"] : map genpara [1..]
    genpara n = [ [a,b,c] | i <- [0..(n-1)/3], j <- [i..(n-1-i)/2],
                            a <- para!!i, b <- para!!j, c <- para!!(n-1-i-j)]

    equiv a b = member (equivclass a) b
    equivclass x = closure_under_laws [invert, rotate, swap] [x]

    invert [[a,b,c],d,e,f] = [a,b,c,[d,e,f]]
    '          ("H":x)     = "H":x

    rotate [a,b,c,d] = [b,c,d,a]
    swap   [a,b,c,d] = [b,a,c,d]

    closure_under_laws fs xs = xs ++ closure' fs xs xs
    closure' fs xs ys = closure'' fs xs (nodups [a | f <- fs, a <- map f ys,
                                                     not (member a xs) ])
    closure'' fs xs [] = []
    '               ys = ys ++ closure' fs (xs ++ ys) ys

    quotient f []   = []
    '         (a:x) = a : [ b | b <- quotient f x, not (f a b) ]
\end{verbatim}

\section{A Doctor's Office}

\begin{quotation}
``Given a set of patients, a set of doctors, and a receptionist, model
the following interactions: Initially, all patients are well, and all
doctors are in a queue awaiting sick patients.  At random times,
patients become sick and enter a queue for treatment by one of the
doctors.  The receptionist handles the two queues, assigning patients
to doctors in a first-in-first-out manner.  Once a doctor and patient
are paired, the doctor diagnoses the illness and, in a randomly chosen
period of time, cures the patient.  Then, the doctor and patient
return to the receptionist's desk, where the receptionist records
pertinent information.  The patient is then released until such time
as he or she becomes sick again, and the doctor returns to the queue
to await another patient.

You may use any distribution functions you wish to decide when a patient
becomes  sick  and  how  long a patient sees a doctor, but the code that
models doctors must have no knowledge of the distribution  function  for
patients,  and  vice  versa,  and  that for the receptionist should know
nothing of either.  The receptionist  may  record  any  information  you
wish:  patient's name, doctor assigned, illness, cure, wait times, queue
lengths, etc.  The purpose of  the  problem  is  to  evaluate  how  each
language expresses asynchronous communications from multiple sources.''
\end{quotation}

Of the four problems, this is probably the least well-defined.  The
main difficulty lies in just what is meant by the verb ``model'' in
the first sentence.  Perhaps the most common kind of modelling is a
simulation of the actual time/event pairs, and that is what the first
solution (written by Joe Fasel) presented below does.  However, such a
solution removes completely the non-determinism and asynchrony of the
problem (since they are being simulated), which conflicts somewhat
with the statement made in the {\em last}\/ sentence of the problem
description.  Thus we also provide a solution that uses explicit
non-determinism.  Unfortunately, non-determinsm is not part of the
Haskell standard, and thus we assume a primitive operator called {\tt
choose} which non-determistically chooses an element from a list.

The two solutions are radically different, and reflect very different
characteristics of Haskell.

\subsection{Time/event Simulation}

This model of the doctors' office takes as input a number of patients,
a number of doctors, an initial list of times at which patients get
sick, and two infinite lists of durations, representing the
distributions of times that patients remain well and of the times
doctors take to cure patients.  An infinite list of tuples is
returned, containing the following information for each office visit:
\begin{verbatim}
     (patient, sick-time, doctor, start-treatment-time, cure-time)
\end{verbatim}
That is, a patient number, the time the patient got sick and entered
the patient queue, the number of the doctor assigned, the time at
which the patient was assigned a doctor, and the time the doctor
finished treating the patient.

The style of this solution is to create mutually recursive streams of
time/event pairs, merging them together at appropriate places while
preserving the temporal order.  The main streams of events are
patients ({\tt patientQ}), doctors ({\tt doctorQ}), and cured people
({\tt cured}), as shown below.  {\tt insert} and {\tt makeQ} are
utilities for handling queues of time-event pairs.

\begin{verbatim}
  doctors n m initialWellDist WellDist CureDist = cured

  where insert y    [] = [y]         -- insert y into time-ordered queue
        '   (p',t') rest@((p,t):xs) | t'<t = (p',t') : rest
        '                                  = (p,t)   : insert (p',t') xs

        makeQ (x:xs) yys             -- initial queue (in order),
                                     -- subsequent entries (not in order)
            = x : makeQ (insert y xs) ys where y:ys = yys
    

        patientQ                     -- [(patient, sick-time)]
            = makeQ (foldr insert [] (zip [1..n] initialWellDist))
                    [(p,c+x) | ((p,s,d,t,c),x) <- zip cured wellDist]
    

        doctorQ                      -- [(doctor, time-available)]
            = makeQ [(d,0) | d <- [1..m]]
                    [(d,c) | (p,s,d,t,c) <- cured]
        cured
            = [(p,s,d,t,t+x) where t = max s a
               | ((p,s),(d,a),x) <- zip3 patientQ doctorQ cureDist]
\end{verbatim}

\subsection{Asynchronous Process Model}

In the following solution the ``world'' is modelled as a 6-tuple:
\begin{verbatim}
    [healthy_people, -- list of healthy people
     sick_people,    -- queue of sick people
     being_cured,    -- list of sick-people/doctor pairs
     cured_people,   -- queue of cured-people/doctor pairs
     doctor_q,       -- queue of available doctors
     record]         -- receptionist's record of pertinent data
\end{verbatim}
This representation is actually more detailed, and thus more
realistic, than the previous one.  In particular, note the presence of
a record book, as well as a queue to hold the doctor/patient pairs
reporting back to the receptionist after a curing session (this queue
is not called for in the specification, but seems more realistic).
The initial state of the world should be obvious:
\begin{verbatim}
    initial_world = ([1..n], -- everybody's healthy     
                     [],     -- nobody's sick           
                     [],     -- nobody's being cured    
                     [],     -- nobody's just been cured
                     [1..m], -- every doctor is idle    
                     [])     -- no record of curing     
\end{verbatim}

The dynamics of this model are captured by three ``processes'' that
operate non-deter\-ministically (i.e.\ asynchronously) and in
parallel.  Each process takes as input a world and outputs a ``new''
world.  Simulation of the doctors office proceeds by starting with the
initial world and iteratively choosing a process non-deterministically
with which to generate a new world on each step of the simulation.
The result is an infinite stream of worlds.

\begin{verbatim}
  doctors world = choose_loop world processes

  processes = [sickening_process, curing_process, receptionist]

  sickening_process w@([],s,b,c,d,r) = w              -- everybody's sick!!
  '                 w@(h, s,b,c,d,r) = (hs,p:s,b,c,d,r)
                                       where (p,hs) = sicken_one h

  curing_process w@(h,s,[],c,d,r) = w                 -- nobody being cured
  '              w@(h,s,b, c,d,r) = (h,s,dps,dp:c,d,r)
                                    where (dp,dps) = cure_one b
                         

  receptionist w = choose [help_the_sick,move_the_cured] w
    where help_the_sick w@(h,[],  b,c,d,   r) = w     -- nobody's sick
          '             w@(h,s,   b,c,[],  r) = w     -- no free doctors
          '             w@(h,p:ss,b,c,d:ds,r) = (h,ss,(d,p):b,c,ds,r)

          move_the_cured w@(h,s,b,[],       ds,r) = w -- no recent curing
          '              w@(h,s,b,(d,p):dps,ds,r) = 
                                            (p:h,s,b,dps,ds++[d],(d,p):r)

  cure_one   = choose_and_remove   -- random curing function
  sicken_one = choose_and_remove   -- random sickening function

  choose_and_remove lst = (el, [y | y<-lst, y\=el])
                          where el = choose lst

  choose_loop obj fs = new_obj : choose_loop new_obj fs
                       where new_obj = choose fs obj
\end{verbatim}
Note that the non-deterministic utility functions are built from a
single non-deterministic primitive called {\tt choose} that
non-deterministically selects an element from a list.

This non-deterministic process model, by the way, could be made
deterministic by providing lists of sickness and wellness
distributions as in the time/event simulation.  Similarly, the
time/event simulation could be made non-deterministic by suitably
merging the event streams non-deterministically.

\section{Skyline Matrix Solver}

\begin{quotation}
``Solve the system of linear equations:
\[ A\ x = b \]
where $A$ is an $n$ by $n$ skyline matrix.  A skyline matrix has
nonzero elements in column $j$ in rows $i$ through $j$, $1 \leq i \leq
j$, and has nonzero elements in row $i$ in columns $j$ through $i$, $1
\leq j \leq i$.  The first constraint defines the skyline above the
diagonal, which is towards the top, and the second constraint defines
the skyline below the diagonal, which is towards the left.  For
example, if
\[ A\ = \left[
       \begin{array}{ccccccc}
       X & 0 & 0 & x & 0 & 0 & 0 \\
       0 & X & 0 & x & 0 & x & 0 \\
       0 & x & X & x & x & x & 0 \\
       0 & 0 & 0 & X & x & x & 0 \\
       0 & x & x & x & X & x & 0 \\
       0 & 0 & 0 & x & x & X & 0 \\
       0 & 0 & 0 & 0 & 0 & 0 & X 

       \end{array}
       \right]
\]
then the $i$ vector of the first constraint is:
\[ \ab{1,2,3,1,3,2,7} \]
and the $j$ vector of the second constraint is:
\[ \ab{1,2,2,4,2,4,7} \]
You may assume any input form for $A$ and $b$ you wish, and you may
assume the $i$ and $j$ vectors as input parameters.  A rather obscure
reference for the problem (available on request) is:
\begin{quotation}
Eisenstat, S.C., and Sherman, A.H. {\em Subroutines for envelope solution  
of 
sparse linear systems}.  Research Report 35, Yale University, New Haven  
CT, 
October 1974.
\end{quotation}
The intention of this problem is to test each language's ability to
manipulate arrays, to use the structure of arrays to avoid unnecessary
computations, and to express array operations.''
\end{quotation}

Our understanding of this problem was aided greatly not only by the
above tech report, but also a copy of some Fortran code written by
Andy Sherman which implements an envelope method for solving a linear
system.  That code, complete with documentation, is also listed in the
Appendix of \cite{huda88f}.

Having Sherman's code provided us with an opportunity to study
Fortran-style {\em incremental} array manipulations in a functional
language, and to contrast that with the preferred {\em monolithic}
array approach.  We think the results are quite interesting.  To
conduct the study we first converted, as faithfully as possible, the
Fortran code into Haskell using incremental updates to purely
functional arrays (see \cite{huda87b} for a discussion of incremental
arrays).  We then rewrote the program in a monolithic style, adhering
more closely to the matrix algebra, but using the same envelope
representations used by Sherman.

The incremental functional array solution is presented in
\cite{huda88f}, and illustrates how one {\em could} do incremental
array operations in a functional language that ``have the feel'' of
side effects to arrays in an imperative language.  In fact, the
incremental program, when run on our Alpha-Tau implementation of Alfl
\cite{huda84b}, achieves the same space complexity as the Fortran
program.  That is, our optimizer is able to infer that every array is
``single-threaded'' and thus updates can be done destructively rather
than by copying.

On the other hand, this is not the {\em preferred} way to program with
arrays in a functional language.  Haskell has a primitive data type
for arrays together with special syntax that allows the specification
of an array instance {\em monolithically} rather than incrementally.
That is, the entire final array is specified in one monolithic
declaration, yielding a declarative reading more in line with the
philosophy of functional programming.  This style of solution is
presented below.

\newcommand {\bmba}[1]  { \[ \begin{array}{#1} }
\newcommand {\eaem}     { \end{array} \] }
\newcommand {\belongs}  { \epsilon }

\subsection{Monolithic Array Solution}

The skyline problem illustrates well some of the special strengths of
Haskell arrays.  In particular, the array specifications can be
derived from the original mathematical definition of the problem in a
clear and straightforward way.  The essential data dependences are
clear, rather than obscured by extraneous operational sequencing.  The
recursive definition of arrays, including mutually recursive
definitions of multiple arrays, permit elegant specifications as well
as efficient implementations.  Haskell arrays permit separate
definitions for elements in different regions of an array, which
permits optimizations similar to the lifting of computations from
Fortran loops, and which clearly correspond to the mathematical
function domain specifications.

The incremental solution was essentially a transcribed version of
Sherman's code, and thus we included no description of the data
representations or the algorithm.  For the monolithic solution we will
instead start from the very basics, and develop the final program via
step-wise refinement of the specification.

\subsubsection{Introduction to Sherman's envelope format for sparse  
matrices.}

Sherman's envelope format works best when the sparse linear system
{\tt A*x = b} has its equations and variables ordered such that most
of {\tt A}'s nonzeros are close to the main diagonal.  Each row {\tt
i} of the lower triangle is stored as an envelope from the leftmost
nonzero in the row up to the last column {\tt j = i-1} before the
diagonal.  Likewise, each column {\tt j} of the upper triangle is
stored as an envelope from the uppermost nonzero in the column down to
the last row {\tt i = j-1} before the diagonal.  The main diagonal
itself is stored as a 1-D vector of length {\tt n}.

Sherman represents a sparse matrix as the 5-tuple {\tt (n, pl, d, pu,
irl, iru)} where:
\begin{itemize}
\item   {\tt n} = the order of {\tt A}.

\item   {\tt pl, d, pu} = 1-D floating point vectors representing
        the lower triangle's consecutively stored row envelopes,
        the main diagonal elements,
        and the upper triangle's consecutively stored column envelopes.

\item   {\tt irl, iru} = 1-D length {\tt n} integer vectors of
        base addresses into {\tt pl} and {\tt pu}.
\end{itemize}

    The base address vectors require some explanation.  For  access into
lower triangle {\tt pl}, suppose we defined the vectors:
\begin{itemize}
\item   {\tt fl!i} = the column index of the first nonzero in row {\tt i};
\item   {\tt begin\_l!i} = the index into {\tt pl} of row {\tt i}'s first
        nonzero.
\end{itemize}
Then we would access {\tt a!(i,j)} in the lower triangle by
{\tt pl!(begin\_l!i + j - fl!i)  =  a!(i,j).}
But the value {\tt begin\_l!i - fl!i} is the same for every {\tt j}
in row {\tt i}.  In a later section we will see that computing an element
{\tt (i,j)} of either the lower or upper triangle factor requires an
inner product summation that runs along the lower triangle's row {\tt i}
and the upper triangle's column {\tt j}.  For a sequential program
it is desirable to make this summation the innermost loop to preserve
locality of reference and therefore achieve good cache and virtual
memory hit rates.  Therefore we would like to raise this
loop-invariant computation out of the innermost loop,
replacing the $O(n^2)$ evaluations of the expression
{\tt begin\_l!i - fl!i} by $O(n)$ evaluations.
We also save space in the representation
by replacing the $2$ length-$n$ vectors with a single length-$n$ vector.
\begin{verbatim}
    irl!i           =  begin_l!i - fl!i
    pl!(irl!i + j)  =  a!(i,j).
\end{verbatim}
The value {\tt irl!i} can be thought of as the row {\tt i} envelope's
base address into {\tt pl}.  The ``first nonzero" function {\tt fl} is
useful as a limit for the summation over all {\tt j} in row {\tt i},
but can be easily recovered from {\tt irl}. The upper triangle's
column-oriented envelopes are stored in a similar fashion.

\subsubsection{The ``first nonzero" functions.}

We will show how the first nonzero function {\tt fl} for the row-oriented
envelopes in the lower triangle can be recovered from the vector {\tt  
irl}.
A similar function {\tt fu} can be derived for the column-oriented
upper triangle envelopes.

The last column stored for row {\tt i} is {\tt j = i-1}.  Let {\tt
(pl\_env\_len i)} = the row {\tt i} envelope size.  Then the column
index of row {\tt i}'s first nonzero is
\begin{verbatim}
    fl i  =  i - (pl_env_len i).
\end{verbatim}
The index values {\tt (irl!i-1 + i-2)} and {\tt (irl!i + i-1)} into {\tt
pl} point to the end of the row {\tt i-1} and row {\tt i} envelopes
respectively.  Since the envelopes are stored consecutively in {\tt
pl}, we have
\begin{verbatim}
    pl_env_len i  =  (irl!i + i-1) - (irl!(i-1) + i-2),
\end{verbatim}
therefore,
\begin{verbatim}
    fl i  =  i - 1 + irl!i-1 - irl!i.
\end{verbatim}

Since there is no {\tt irl!0} entry, {\tt fl} is only defined for
{\tt i <- [2..n]},   The row 1 envelope is always empty in the lower
triangle.  For an empty row the first nonzero is in column {\tt (fl i) =  
i};
so the envelope contains columns {\tt j <- [(fl i)..(i-1)] =} $\phi$.

We could put a conditional in {\tt fl} to make it defined for row $1$,
but this imposes a run-time test for every row.  A better alternative
is to define a bogus {\tt irl!0} that causes {\tt (fl 1)} to return $1$.
Entry {\tt irl!1} always has the value
{\tt irl!1 = (begin\_l 1) - (fl 1) = 1 - 1 = 0},
therefore {\tt irl!0} must satisfy the equation:
\begin{verbatim}
    1  =  fl 1  =  1 - 1 + irl!0 - irl!1  =  irl!0.
\end{verbatim}
However, we will discover later that we can always avoid any calls to
{\tt fl} for row 1, or to {\tt fu} for column 1.


\subsubsection{A functional derivation of $L*U$ factorization.}

The problem is to solve the linear system $A*x = b$: given $A$
and $b$, what is $x$?  If $A$ is invertible, there exists
a unique factorization $A = L*U$ where $L$ is lower triangular
and $U$ is unit upper triangular, which reduces the original
problem to the easier problem of solving the triangular linear systems
$L*y = b,\ U*x = y$.

But this leaves the problem: given $A$, what are $L$ and $U$?
The usual derivation of $L$ and $U$ is presented as a
sequence of steps $k\ \belongs\ [1..n]$, each step forming an intermediate
matrix $A(k)$; this particular sequential approach to Gaussian
elimination is very obscure, hiding the essential data dependences
under non-essential operational details.  Instead we will first write
out the equation $A = L*U$ as if we were finding $A$
given $L$ and $U$, then by algebraic manipulation, derive
mutually recursive equations for $L$ and $U$ given $A$.
We will see that the Haskell program mimics closely the mathematical
notation we use to derive the equations for $L$ and $U$.  Because
of this close resemblance, the program is easy to reason about, the
essential data dependences are clear, and it is easy to justify and
debug optimizations.

Each $a(i,j)$ is the inner product of $l$'s row $i$ and $u$'s
column $j$:
$ 
    a(i,j)\ =\ \sum_{k=1}^n l(i,k)*u(k,j),
    \ \ i\ \belongs\ [1..n], \ j\ \belongs\ [1..n]
$.
But there is no contribution to $a(i,j)$ for terms in which $l(i,k) = 0$
(for columns $k$ to the right of the diagonal: $i < k$) or in which
$u(k,j) = 0$ (for rows $k$ below the diagonal: $j < k$).  Therefore,
instead of summing over $k\ \belongs \ [1..n]$, we only need to sum over
$k\ \belongs \ [1..(min\ i\ j)]$.

Equivalently, we can separate the definitions for $a(i,j)$ in the
lower triangle and diagonal ($i \leq j$, and therefore use $j$
as the summation limit):
\bmba{rlll}
    a(i,j)  & =  \sum_{k=1}^{j} l(i,k)*u(k,j)   &                   \\
            & =  l(i,j)*u(j,j) + \sum_{k=1}^{j-1} l(i,k)*u(k,j),
                & \ \ i \ \belongs \ [1..n],\ j \ \belongs \ [1..i],    \\
\eaem
or in the upper triangle ($i < j$, and therefore use $i$ as 
the summation limit).
\bmba{rlll}
    a(i,j)  & =  \sum_{k=1}^{i} l(i,k)*u(k,j)   &                   \\
            & =  l(i,i)*u(i,j) + \sum_{k=1}^{i-1} l(i,k)*u(k,j),
                & \ \ i \ \belongs \ [1..n],\ j \ \belongs \ [i+1..n].
\eaem

But we can immediately rearrange these equations to define the elements
of $L$ and $U$ (recall that we require $u(j,j) = 1.0$ for all $j$):
\bmba{rll}
    l(i,j)
            & =  a(i,j) - \sum_{k=1}^{j-1} l(i,k)*u(k,j),
                & \ \ i \ \belongs \ [1..n],\ j \ \belongs \ [1..i],    \\
    \\
    u(i,j)
            & =  (\ a(i,j) - \sum_{k=1}^{i-1} l(i,k)*u(k,j)\ ) \ / \ l(i,i),
                & \ \ i \ \belongs \ [1..n],\ j \ \belongs \ [i+1..n].
\eaem

The $L$ equations show us that whatever the operational sequencing,
$l(i,j)$ depends on $a(i,j)$ and recursively depends on
other $L$ elements in the same row $i$ and to the left,
and on $U$ elements in the same column $j$ and above. 

The recursion terminates upon the leftmost column of $L$ and the
topmost row of $U$.  Similar reasoning holds for the $U$
equation.

In the following we will replace $l(i,i)$ with the name $d(i)$.
There are optimizations we can perform on $L$'s diagonal
elements that make them deserve special treatment.  The $d(i)$'s
are {\em not} to be confused with the elements of diagonal matrix
$D$ in the $L*D*U$ factorization, where both $L$ and $U$
are unit triangular.

\subsubsection{$L*U$ factorization in dense array format.}

For clarity, we introduce some new syntax into list comprehensions.
In an {\tt array} function's list comprehension argument, the {\tt
(i,x)} pair can be written in the form {\tt i = x}, similar to the
declarations in a Haskell {\tt where}.  For example,
\begin{verbatim}
    array ((1,1), (N,N))
      [ (i,j) = k*a!(i,j) | i <- [1..N], j <- [1..N] ]
\end{verbatim}

>From the equations in the previous section let us write a functional
program to compute $L$ and $U$.  Let us define a higher-order function
for the mathematical summation sign:
\begin{verbatim}
    sum i j accum f  =  if j<i then accum else sum (i+1) j (accum + (f i)) f
\end{verbatim}
Let us define a function {\tt l} that given index {\tt (i,j)},
computes the element value in $L$:
\begin{verbatim}
    l (i,j)  =  a!(i,j) - (sum 1 (j-1) 0. l_exp)
        where l_exp k  =  L!(i,k) * U!(k,j)
\end{verbatim}
This definition of {\tt l} is very similar to a DO loop in Fortran,
and can be compiled as efficiently.

Instead we will use an alternative definition of {\tt sum}
that operates over a list.
\begin{verbatim}
    sum xs  =  sum1 0. xs
    where
        sum1 accum []      =  accum
        sum1 accum (x:xs)  =  sum1 (accum+x) xs
\end{verbatim}
Used with a list comprehension argument,
this version gives a somewhat more legible way of
writing {\tt l i j}, and more closely resembles the mathematical
summation sign.  
\begin{verbatim}
    l (i,j)  =  a!(i,j) - sum [ L!(i,k) * U!(k,j) | k <- [1..j-1] ]
\end{verbatim}
We can think of the list as a multiset, and of {\tt sum} as summing
the elements of the set (although strictly speaking, floating point
addition is not associative).  Techniques such as Wadler's
listlessness and deforestation transformations can ensure that such an
expression gets converted to a semantically equivalent expression in
which the lists are eliminated; the expression can be compiled as
efficiently as a DO loop (\cite{wadl84,wadl85b}).

This definition of {\tt l} holds for {\tt i <- [2..n], j <- [1..i-1]}.
Row 1 is skipped since {\tt l (1,1)} is on the diagonal and we wish to
define the diagonal elements separately.

    The definition for {\tt d}, which computes $L$'s diagonal elements,
is a simplified version of the definition for {\tt l} above
(since {\tt i = j}),  for {\tt i <- [1..n]}.

The definition for $U$ is nearly the same as for $L$ except that summation
stops at {\tt k = i-1}.  Then the entire row is scaled by
{\tt 1./d!i} to normalize $U$'s diagonal to 1.0.
$U$'s definition holds for {\tt i <- [1..n-1], j <- [i+1..n]},
or equivalently, for {\tt j <- [2..n], i <- [1..j-1]}.  See the
complete program at the end of this section.

    Each element {\tt d!i} appears as a divisor in the definition for  
every
{\tt u!(i,j)} in the same row {\tt i} ($(n^2 - n)/2$ divisions  
altogether),
as well as in definition of {\tt x!i} in the same row for the $L*U*x = b$
backsolve stage ($n$ divisions).  Since division is expensive compared  
with
multiplication, we instead store the inverse of each {\tt d!i}, replacing
$O(n^2)$ divisions with $n$ divisions and $O(n^2)$ multiplications.
This is a classic example of using an array to store expensive shared
computations.

    The definitions of $L$ and $U$ are mutually recursive, both
in the mathematical definition and in the Haskell array definition.
We do not need to store $L$'s upper or $U$'s lower triangle, which
are zero, or $U$'s unit diagonal, so we can store all the essential
results in a single $n^2$ array.
We can recursively define the matrix {\tt lu}  $= (L-D) + D^{-1} + (U-I)$
in dense matrix format (where $D$ = $L$'s diagonal,
$D^{-1} = D$'s inverse):
\begin{verbatim}
    plu a  =  lu
    where
        ((1,1),(n,n)) = bounds a
        lu = array ((1,1),(n,n))
                [ (i,j) = l i j  | i <- [2..n], j <- [1..i-1] ]  ++
                [ (i,i) = d i    | i <- [1..n] ]  ++
                [ (i,j) = u i j  | i <- [1..n-1], j <- [i+1..n] ]
        l i j  =  a!(i,j) - sum [ lu!(i,k) * lu!(k,j) | k <- [1..j-1] ]
        d i    =  1./s
            where s = a!(i,i) - sum [ lu!(i,k) * lu!(k,i) | k <- [1..i-1] ]
        u i j  =  lu!(i,i) * ( a!(i,j)
                    - sum [ lu!(i,k) * lu!(k,j) | k <- [1..i-1] ] )
\end{verbatim}


\subsubsection {Lazy arrays and strict arrays}

Notice that the domain specifications in the the array comprehension
correspond exactly to the domain specifications given in the mathematical
function definitions.  These domain specifications should {\em not} be
thought of as looping constructs: they say nothing about the order in
which elements of the array {\tt lu} should be evaluated.

In fact, we could let {\tt lu} be a {\em lazy array}.   Each element
in a lazy array is represented by a thunk which is evaluated only when
demanded.  Domain specifications such as {\tt i <- [2..n], j <- [1..i-1]}
specify {\em where} thunks for a particular form of expression must be
placed, but say nothing about the {\em order} in which the thunks are
evaluated.

Evaluation of an element in a lazy array is forced only when it is
explicitly requested.  If a lazy array is recursively defined,
evaluation of an element in turn forces the evaluation of other
elements on which it has a data dependence.

But if an element has already been forced once, the thunk modifies
itself so that its value is returned immediately, without recomputation.
We can think of an array {\tt a} as a function of {\tt d} integer  
arguments
(where {\tt d} is the the array's dimension), for which we know that
any given function application ({\tt a }$\ i_1\ ...\ i_d$) (i.e.,
any given array element {\tt a!}$(i_1,\ ...,\ i_d)$), will be requested
many times.  In this view an array is a {\em caching function}.

There are several essential differences between lazy arrays in Haskell
and arrays in a language like Fortran.  Haskell specifies the result
array monolithically in terms of a definition for each element,
whereas Fortran specifies the result array in terms of incremental
updates to the input array.  For the example program presented so far,
Haskell's monolithic definition requires that the output array be
computed in a separate space from the input array.

For Haskell to be able to reuse the input array {\tt a} to store the
output array {\tt lu}, the compiler must know that reuse is safe.
There must no other outstanding references to {\tt a} outside the
definition of {\tt lu}.  Furthermore, an element {\tt a!(i,j)} must
be dead at the time that it is replaced by element {\tt lu!(i,j)},
which means that either the compiler must determine or the programmer
must specify a safe order of evaluation.  This research topic is 
the subject of \cite{ande89} and \cite{ande89b}.

Another difference is that the Fortran programmer must be careful to
arrange the order of his computation so that whenever
he evaluates an element {\tt lu!(i,j)}, the elements on which {\tt  
lu!(i,j)}
has a direct data dependence will have already been computed.
Both the Haskell and the Fortran arrays can be viewed as cached
functions, so although they may differ in the {\em order} in which
array elements are evaluated, there is no difference in the {\em total
amount} of computation time spent on array indexing and floating point
arithmetic.  But Haskell's thunks increase the time by a small constant
factor. In addition to computing the element values, we must also create
a thunk for each element when array storage is allocated; and whenever
an element is demanded we must test whether or not its thunk as been
forced yet.

Notice that we could eliminate the need for creating and testing element
thunks if, like the Fortran programmer, we could guarantee a safe order
of evaluation.  See \cite{ande89,ande89b}.


\subsubsection{Refinement of $L*U$ factorization using ``first
               nonzero'' information.}

Notice that in the summations, the {\tt k}-th term {\tt l!(i,k) * u!(k,j)}
makes no contribution if either factor is zero.  If a sparse matrix
is organized such that most nonzeros are close to the main diagonal,
considerable work can be saved by ignoring terms in which
either {\tt l!(i,k)} falls to the left of row {\tt i}'s first nonzero
or if {\tt u!(k,j)} falls above column {\tt j}'s first nonzero.
In other words, skip terms for which {\tt k < (fl i)} or {\tt k < (fu  
j)}).

Assume we are given the ``first nonzero"
functions {\tt fl} and {\tt fu}.  {\tt L} is then defined by:
\begin{verbatim}
    l (i,j)  =  a!(i,j) - sum [ lu!(i,k) * lu!(k,j) } | k <- [kmin_l..j-1] ]
    where kmin_l   =  max (fl i) (fu j)
\end{verbatim}

We have defined {\tt l (i,j)} for {\tt i <- [2..n], j <- [1..(i-1)]}.
But recall the domains of the first nonzero functions:
{\tt (fl i)} is defined over {\tt i <- [2..n]}, which causes
no problem, but {\tt (fu j)} is defined only over {\tt j <- [2..n]},
which makes {\tt (l (i,j))} undefined in column $1$.

We can put a run-time test in {\tt (fu j)} for the case column
{\tt j = 1}, but then this test gets executed for every one of
the $O(n^2)$ lower triangle elements.  But we notice that whenever
{\tt j} is at or to the left of the row's first nonzero (i.e.,
when {\tt j <= (fl i)}), the summation must terminate immediately.
Then we are left with {\tt (l (i,j)) = a!(i,j)}, which is zero for
{\tt j < (fl i)}, nonzero for {\tt j = (fl i)}.

Therefore, when we know {\tt j <= (fl i)}, we can return {\tt a!(i,j)}
immediately, avoiding calls to the summation altogether.  This case
includes column {\tt j = 1}, so we do not need special treatment
for this column.

We partition the lower triangle into different regions
which use separately tailored element definitions.
\begin{verbatim}
    lu = array ((1,1),(n,n))
            [ (i,j) = l i j  | i <- [2..n], j <- [(fl i)+1..i-1] ]  ++
            [ (i,j) = a[i,j] | i <- [2..n],
                                j <- [ (fl i) ], j < i ]  ++
            [ (i,j) = 0.0    | i <- [2..n], j <- [1..(fl i)-1] ]
            . . .
\end{verbatim}
Similar partitionings hold for the diagonal and upper triangle.
These clauses partition the lower triangle into three regions:
\begin{enumerate}
\item   {\tt  j <- [(fl i)+1..i-1] } :
        inside the envelope except for the envelope's first column
        (the empty set if first nonzero is on or immediately beside
        the diagonal),

\item    {\tt  j <- [ (fl i) ], j < i } :
        the envelope's first column
        (the empty set if first nonzero is on the diagonal).

\item    {\tt  j <- [1..(fl i)-1] } :
        outside the envelope
        (the empty set if first nonzero is in column 1).
        These zero elements of {\tt a} are guaranteed not to fill in
        for {\tt lu}.
\end{enumerate}


\subsubsection{$L*U$ factorization using envelope representation.}

Finally let us convert our definitions from dense format to envelope
format.  Let input matrix {\tt a} be represented by the tuple {\tt (n,
old\_pl, old\_d, old\_pu, irl, iru)} and result matrix {\tt lu} be
represented by {\tt (n, new\_pl, new\_d, new\_pu, irl, iru)}.

The auxiliary vectors {\tt iru} and {\tt irl} are the same for
both input matrix {\tt a} and output matrix {\tt lu}.  Zero elements
in {\tt a} that become non-zero in {\tt lu} are called {\em fill-in}.
Fill-in can occur only inside the envelope; all zeroes outside
{\tt a}'s envelope are guaranteed to remain zero in {\tt lu} and
are therefore also outside {\tt lu}'s envelope; therefore {\tt a}
and {\tt lu} have the same envelope structure, as represented
by vectors {\tt iru} and {\tt irl}.

The vector bounds for {\tt new\_d} are simply {\tt (1,n)}.
We can get the new vector bounds for {\tt new\_pl} (and similarly
for {\tt new\_pu} by fetching the old vector bounds: {\tt (bounds pl)}.
Alternatively, we can observe that the index of {\tt pl}'s last element
is {\tt irl!n + n - 1}.

Example substitutions for references to the input matrix {\tt a} are:
\begin{list}{}
    \item When {\tt i <- [2..n], j <- [(fl i)..(i-1)]},     \\
        {\tt a!(i,j)} becomes {\tt old\_pl!(irl!i + j)}.
    \item When {\tt i <- [1..n]},                           \\
        {\tt a!(i,i)} becomes {\tt old\_d!(i)}.
    \item When {\tt j <- [2..n], i <- [(fu j)..(j-1)]},     \\
        {\tt a!(i,j)} becomes {\tt old\_pu!(i + iru!j)}.
\end{list}
Similar substitutions can be made for references to
{\tt l}, {\tt d}, and {\tt u} regions of {\tt lu!(i,j)}.
The definition of the lower triangle then becomes:
\begin{verbatim}
    l (i,j)   = s
        where
            kmin_l      =  max (fl i) (fu j)
            l_exp k     =  new_pl!(irl!i + k) * new_pu!(k + iru!j)
            accum_init  =  old_pl!(irl!i + j)
            s           = a_init - sum [ l_exp k | k <- [kmin_l..j-1] ]

    new_pl  = array (bounds old_pl)
                [ irl!i+j  =  l (i,j)
                        | i <- [2..n], j <- [(fl i)+1..i-1] ]  ++
                [ irl!i + j  =  old_pl!(irl!i + j)
                        | i <- [2..n], j <- [(fl i)], j < i ]
\end{verbatim}

There are many opportunities for common subexpression
elimination.  For a given element {\tt (l (i,j))}, every term {\tt k}
in the summation uses the same base address values {\tt irl!i} and
{\tt iru!j}, so we could save 2 vector lookups per term.

Also notice that for a given row {\tt i}, the expressions
{\tt irl!i} and {\tt (fl i)} appear both in the definition of
{\tt (l (i,j))} and in the array comprehension for {\tt new\_pl}.
By abstracting these two expressions out of the definition of
{\tt (l (i,j))} and computing them at the level of the array  
comprehension,
we not only share between these two parts of the program, we also
ensure that these expressions are computed only once for a given row,
instead of getting recomputed for each element in the row.

Here is our final complete version of the $L*U$ factorization,
giving the code for the lower triangle.  The code for the main
diagonal and the upper triangle are similar.

\begin{verbatim}
    plu (n, old_pl, old_d, old_pu, irl, iru)  =
        (n, new_pl, new_d, new_pu, irl, iru)
    where
        l (i,j) irli fli = s
        where
            kmin_l  = max fli (fu j)
            iruj    = iru!j
            l_exp k = new_pl!(irli + k) * new_pu!(k + iruj)
            a_init  = old_pl!(irli + j)
            s       = a_init - sum [ l_exp k | k <- [kmin_l..j-1] ]
        new_pl  = array (bounds old_pl)
                    [ (irli + j)  =  l (i,j) irli fli
                        | i <- [2..n],
                                irli <- [ irl!i ],
                                fli  <- [ (fl i) ] ,
                                j <- [fli+1..i-1] ]  ++
                    [ (irli + j)  =  old_pl!(irli + j)
                        | i <- [2..n],
                                irli <- [ irl!i ],
                                fli  <- [ (fl i) ],
                                j <- [ fli ], fli < i ] ]
        . . .
\end{verbatim}

An extension of the list comprehension that treats the nesting of
generators and loop-invariant subexpressions more clearly and
elegantly is discussed in \cite{ande89,ande89b}.

If the compiler treats {\tt i} as an outer loop index and {\tt j} as
an inner loop index, we have achieved the equivalent of lifting
loop-invariant computations in Fortran.  There remain a few more
opportunities for lifting common subexpressions in this program
fragment, but we have taken all opportunities that lift
loop-invariant subexpressions.

\subsubsection{The $L*U*x = b$ solution phase.}

We will discuss two versions of the backsolve phase, one version
using the column-oriented {\tt U}, the other using the reorganized
row-oriented {\tt U}.

$A*x = L*U*x = b$ is equivalent to solving the lower triangular
system $L*y = b$ , using the intermediate solution $y$ as the
righthand side for solving the upper triangular system $U*x = y$.
\begin{verbatim}
    l!(1,1)*(y 1)                                       = b!1
    l!(2,1)*(y 1) + l!(2,2)*(y 2)                       = b!2
    . . .                                               . . .
    l!(n,1)*(y 1) +       . . .       + l!(n,n)*(y n)   = b!n
\end{verbatim}
Recall that we are storing the inverse of diagonal elements under the
name {\tt d!i = 1./l!(i,i)} for every {\tt i}.  For a typical row
{\tt i}, this system of equations can be recast as the function:
\begin{verbatim}
    y_vec  =  array (1,n) [ i = y i  | i <- [1..n] ]
    y i  =  s * d!i
    where s  =  b!i - sum [ l!(i,j) * y_vec!j | j <- [1..i-1] ]
\end{verbatim}

Finally we solve the upper triangular system $U*x = y$, recalling again
that $U$ is unit diagonal.
The entire function, assuming the matrix {\tt lu} $= (L-D) + D^{-1} +
(U-I)$ is in dense matrix format, and doing the appropriate substitutions
for {\tt l!(i,j)}, {\tt d!i}, and {\tt u!(i,j)}:
\begin{verbatim}
    plub lu b  =  x_vec  where
        y i    =  s * lu!(i,i)
            where s  =  b!i - sum [ lu!(i,j) * y_vec!j | j <- [1..i-1] ]
        x i    =  y_vec!i - sum [ lu!(i,j) * x_vec!j | j <- [i+1..n] ]
        y_vec  =  array (1,n) [ i = y i  | i <- [1..n] ]
        x_vec  =  array (1,n) [ i = x i  | i <- [1..n] ]
\end{verbatim}

Now let us stay with dense format, but incorporate the ``first
nonzero" functions {\tt fl} and {\tt fu} to avoid multiplies and
subtracts for terms in which the contribution weight {\tt l!(i,j)} or
{\tt u!(i,j)} is outside the matrix envelope and therefore zero.  The
change is trivial for the lower triangular system, since the summation
is along a row.  For {\tt i <- [2..n]}, {\tt (fl i)} tells us the first
nonzero column in row {\tt i}:
\begin{verbatim}
    y i  =  s * lu!(i,i)
    where s  =  b!i - sum [ lu!(i,j) * y_vec!j | j <- [(fl i)..i-1]
\end{verbatim}
Otherwise {\tt y 1  =  b!1. }

But for the upper triangle we have the problem that the summation is
also running along a row, but {\tt fl} tells us the first nonzero in a  
given
column.  We cannot use it to give a bound on the summation for a row {\tt  
i}
the way we did for the lower triangular system.  We could instead use
{\tt fl} as a predicate for each element of a row to see whether that  
element
falls outside the upper triangle's column-oriented envelope.
\begin{verbatim}
    x i  =  y_vec!i - sum [ lu!(i,j) * x_vec!j
                            | j <- [i+1..n], (fu j) <= i ]
    x_vec  =  array (1,n) [ i = x i  | i <- [1..n] ]
\end{verbatim}
Unfortunately, although we avoid an expensive floating point multiply and
subtract for each zero {\tt u!(i,j)}, we still incur a predicate test for
every element.  The upper triangular envelope may be of size $O(n)$,
but testing {\em every} element for inclusion in the envelope forces us
to perform $O(n^2)$ work.

One approach at this stage is to switch to column-oriented view of
$U$, which was convenient for the factorization phase, to a
row-oriented view more appropriate to the backsolve phase.
\cite{huda88f} gives a program that performs this column-oriented to
row-oriented reorganization of $U$'s envelope representation.  The
reorganization takes time proportional to the size of the upper
triangle's row-oriented envelope.  For matrices in which the maximum
size of any row envelope is independent of matrix size $n$, this time
is $O(n)$.

Another solution is to imitate the Fortran solution, which walks through
$U$ column by column, performing successive updates on the {\tt x}
vector as each {\tt x!j} becomes available.  Keeping for now the dense
representation of {\tt lu}, we can transform the definition of
{\tt xvec} given above to this form:
\begin{verbatim}
    x_vec  =  j_loop n y_vec
    j_loop j x_vec  =
        if j < 1
            then x_vec
            else j_loop (j-1)
                    (array (1,n)
                        [ i = x_vec!i | i <- [1..fuj-1] ] ++
                        [ i = x_vec!i - lu(i,j) * x_vec!j
                                      | i <- [fuj..j-1] ] ++
                        [ i = x_vec!i | i <- [j..n] ])
                    where fuj = fu j
\end{verbatim}
For each iteration of {\tt j} we define a new monolithic array, but we can
easily transform this version to loop over {\tt i} using an
element-at-a-time incremental update function:
{\tt (upd x\_vec i new\_value)}.
Or we can define a function {\tt bigupdate} that has the same
semantics as the {\tt array} expression above, but it is only
necessary to specify the elements that are {\em different} from
{\tt x\_vec}.
\begin{verbatim}
  . . .
  else j_loop (j-1)
              (bigupdate x_vec
                 [ i = x_vec!i - lu(i,j) * x_vec!j | i <- [(fu j)..j-1] ])
\end{verbatim}
A {\tt bigupdate} function could perform an in-place update if the
compiler determines this is safe (see \cite{ande89,ande89b}).
The semantics of {\tt bigupdate} takes a middle ground between the
{\tt upd} function's incremental view of functional arrays and the
{\tt array} constructor's monolithic view.

We finally convert our program to use a envelope format version 
by making the appropriate substitutions to convert references
to {\tt lu} into references to {\tt pl, pu}, and {\tt d}.
Here is the final complete version of the $L*U*x = b$ solver.

\begin{verbatim}
    plub (n, pl, d, pu, irl, iru) b  =  x_vec
    where
        fl i   =  i - 1 + irl!(i-1) - irl!i
        y i    =  s * d!i  where
                s  =  b!i - sum [ pl!(irl!i + j) * y_vec!j
                                       | j <- [(fl i)..i-1] ]
        y_vec  =  array (1,n) [ 1 = b!1 * d!1 ] ++ 
                              [ i  =  y i  |  i <- [2..n] ]

        fu j   =  j - 1 + iru!(j-1) - iru!j
        x_vec  =  j_loop n y_vec
        j_loop j x_vec  =
            if j < 1
                then x_vec
                else j_loop (j-1)
                        (bigupdate x_vec
                            [ i = x_vec!i - pu!(i + iru!j) * x_vec!j
                                          | i <- [(fu j)..j-1] ] )
\end{verbatim}

Andy Sherman's version of {\tt plub} also deals with reordering
the unknowns to achieve a narrower envelope.
The linear system $A*x = b$ may have been poorly organized for the
envelope representation, and the equivalent system $ P*A*P^{-1}*P*x
= P*b $ may require a smaller envelope to store $A$ and its
factorization.  The permutation matrix $P$ reorganizes the rows
($P^{-1}$ the columns) using reverse Cuthill-McKee (RCM) 
heuristic to minimize the envelope size.

Sherman's version of {\tt plub}
assumes that the $LU$-factorized envelope-format matrix is in RCM order,
whereas input vector {\tt b} and result vector {\tt x} are in the original
order.  If we have a vector {\tt iord} representing the permutation $P$
mapping the original number {\tt i} to RCM number {\tt iord!i},
then the change to {\tt plub} is trivial, and is left as an exercise.

\section{Acknowledgements}

We wish to thank Joe Fasel at Los Alamos for comments on various parts
of this document, as well as for providing us with his solution to the
doctors' office problem.  Also thanks to Los Alamos and Lawrence
Livermore National Laboratories for their sponsorship of the Salishan
High-Speed Computing Conference.

\bibliographystyle{alpha}
\bibliography{//alta/yale/hudak/bib/new,//alta/yale/hudak/bib/old}

\end{document}
